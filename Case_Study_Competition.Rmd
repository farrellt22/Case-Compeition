---
title: "Case Study competition"
author: "Thomas Farrell"
date: "3/23/2022"
output: pdf_document
toc : true
font size: 12pt

bibliography: citations.bib
---

\newpage

## Cover Page

Report by:Thomas Farrell, Leo Salazar, Mia Brito, Raneen Aljamal, Adrian Joshua


Primary Contact : Thomas Farrell (512) 400-9580

\newpage


## Executive Summary
|       This data set is collected directly from a fundraising agency and contains 95,412 records broken into 142 fields. The models shown related the efficiency of the different variables to predict if a consumer will donate to the campaign and if they donate it will predict how much they are likely to donate. We will use three different modeling techniques Linear Regression, Linear discriminant analysis(LDA),Decision Tree modeling, and random forest techniques.

The main question to be answered is what type of individual will most likely donate? 
|       The donation can be monitored by factors such as neighborhood code, Their max donation, and the time of their last donation. In the data a donation is represented by the binary number 1. 
## Related Literature: 

|       When it comes to modeling approaches, it is crucial to pick the method that best satisfies your data’s needs. My team members and I have dove into the different types of modeling approaches to explore which method best fits our research. In our past case studies, we first explored logistic regression. As previously mentioned, Mitchell Dayton explains how logistic regression tackles the process of classifying problems. We also explored the use of a linear regression method. We learned that linear regression more commonly handles regression problems. To further enhance our knowledge of the different types of modeling approaches, we will be exploring the use of LDA and Decision Tree models in this case study.
|       Linear Discriminant Analysis, LDA, is another common modeling approach. LDA is commonly used to find a linear combination to separate more than two subjects or events. In Ricciardi, Valente, Edmund, Cantoni, Green, Fiorillo, Picone, Santini, and Cesarelli’s work, they used the LDA model to explore the use of a data mining technique to help clinicians in decision-making when it comes to Coronary Artery Disease. As previously mentioned, LDA models are used to separate two or more subjects. In this case study, they used the LDA model to classify the patients to help aid the decision making process. They concluded that this accelerated and enhanced the decision making process for doctors to choose the correct treatment for their patient. 
|       Similar to an LDA model, a Decision Tree model is used for classification purposes. A Decision Tree model builds regression by creating a model in a tree-like structure. It essentially makes the data set smaller by breaking it down into smaller subsets for an easier and quicker read. Stretch, Soares, and Moreira explored the use of a Decision Tree model when predicting the failure of students in University courses. They used the Decision Tree model to narrow down their data and make it smaller and were able to successfully explain the drop out phenomenon. 




## Data Anlaysis (Methodology)


|        Once we defined our variables in the data set, we changed all of our character variables to factor variables. We then created graphic models of significant variables in our data. This allowed us to visually see what people in our data could be beneficial as we could see which demographics may donate. When making our graphic representations we had to include Donation as the fill for the bar chart or histogram. For the Categorical variables we decided to use a Bar chart to best represent our data. The fill of the "Donation" variable allowed us to show the relationship between the variable and if they would choose to make a Donation or not.

|       We had limitations in the data as some of the variables in the data were highly correlated to Donation. So we had to remove these variables from our main data set. To find these highly correlated variables we ran the findcorrelation function, and set the cutoff to .9.  We then ran a logistical regression and found the GVIF of each variable and ran another logistic regression with only the significant variables.. The rule that we used to determine multicollinearity was if the GVIF was over 5, the variable contained multicollinearity.  We then removed the variables with multiculinqrity we then used the function ovum.sample which can balance overfitted or under-fitted data. 

|       When we achieved our balanced data set we then performed a training and testing split on the data so we could create our models with the testing data.Once we had our data down to the correct variables we were able to see that an optimal cutoff needed to be implemented. We experimented with .1 as the cutoff first and our sensitivity was at .0102 which was telling us that our model was not good at predicting the "yes" responses. We then changed our optimal cutoff to .51 which changed the sensitivity to .588 which gave us a better model.We created an LDA model in our case study which is a modeling technique that reduces the number of variables in our data set while keeping as much information as possible to find a linear combination to predict our categories. The LDA model gave us a sensitivity of .6393 and a specificity of .5276 which showed us that our model was more accurate at predicting “yes” responses. We also observed that our data for this model had an accuracy of .5838 which was slightly higher than that of the logistic regression model. Due to the limitation of not being able to implement an optimal cutoff for the LDA model the accuracy of this model could be seen as false. 
















\newpage

## Appendix

### Loading in libraries
```{r setup, message=FALSE, echo=TRUE}
library(varImp)
library(caret)
library(lattice)
library(ggplot2)
library(gam)
library(caret)
library(ROCR)
library(ggmosaic)
library(prettydoc)
library(tinytex)
library(corrplot)
library(MASS)
library(tidyverse)
library(vip)
library(esquisse)
library(lubridate)
library(modeldata)
library(leaps)
library(ROSE)
library(randomForest)
source('/Users/thomasfarrell/Downloads/optim_threshold.R')

```


[@log1][@log2][@log3][@log4]



### Reading in data


```{r}
b1 = read.csv("/Users/thomasfarrell/Downloads/Customer_Analytics_TrainTest.csv", sep = ",")
```



### Changing character variables to factors

```{r}
b1$State = as.factor(b1$State)
b1$NeighborhoodCode = as.factor(b1$NeighborhoodCode)
b1$Gender = as.factor(b1$Gender)
b1$Donation = as.factor(b1$Donation)
b1 = subset(b1, !is.na(b1$Age))
b1 = subset(b1, !is.na(b1$NumberOfChild))
b1 = subset(b1, !is.na(b1$Income))
b1 = dplyr::select(b1, - PercMale)
b1 = dplyr::select(b1, - PercFemale)
b1 = dplyr::select(b1, - PercWhite)
b1 = dplyr::select(b1, - PercIsleAsian)
b1 = dplyr::select(b1, - AgeC1)
b1 = dplyr::select(b1, - AgeC2)
b1 = dplyr::select(b1, - AgeC3)
b1 = dplyr::select(b1, - AgeC4)
b1 = dplyr::select(b1, - AgeC5)
b1 = dplyr::select(b1, - AgeC6)
b1 = dplyr::select(b1, - AgeC7)
b1 = dplyr::select(b1, - HHN1)
b1 = dplyr::select(b1, - HHN2)
b1 = dplyr::select(b1, - HHN4)
b1 = dplyr::select(b1, - PercMarr)
b1 = dplyr::select(b1, - PercSingle)
b1 = dplyr::select(b1, - MedHomeVal)
b1 = dplyr::select(b1, - MedFamInc)
b1 = dplyr::select(b1, - PerCapInc)
b1 = dplyr::select(b1, - IC2H)
b1 = dplyr::select(b1, - IC1F)
b1 = dplyr::select(b1, - IC2F)
b1 = dplyr::select(b1, - IC3F)
b1 = dplyr::select(b1, - IC4F)
b1 = dplyr::select(b1, - IC5F)
b1 = dplyr::select(b1, - IC6F)
b1 = dplyr::select(b1, - IC9F)
b1 = dplyr::select(b1, - EMP3)
b1 = dplyr::select(b1, - EMP4)
b1 = dplyr::select(b1, - EMP8)
b1 = dplyr::select(b1, - EMP9)
b1 = dplyr::select(b1, - EMP13)
b1 = dplyr::select(b1, - EMP14)
b1 = dplyr::select(b1, - Edu1)
b1 = dplyr::select(b1, - Edu2)
b1 = dplyr::select(b1, - Edu3)
b1 = dplyr::select(b1, - Edu4)
b1 = dplyr::select(b1, - Edu5)
b1 = dplyr::select(b1, - Edu6)
b1 = dplyr::select(b1, - Edu7)
b1 = dplyr::select(b1, - PercForgnBr)
b1 = dplyr::select(b1, - LangEng)
b1 = dplyr::select(b1, - LangSpa)
b1 = dplyr::select(b1, - LangAs)
b1 = dplyr::select(b1, - LangOth)
b1 = dplyr::select(b1, - AmDonated)
b1 = dplyr::select(b1, - NumberOfChild)
b1 = dplyr::select(b1, - PercEmpState)


str(b1)
```


### Removing highly correlated variables

```{r}
b1_num = dplyr::select_if(b1, is.numeric)
M = cor(b1_num)
highcorr = findCorrelation(M, cutoff = .9, names = TRUE)
b1 = dplyr::select(b1, - highcorr)
```




### Checking summary of target variable

```{r}
summary(b1$Donation)
```




### Balancing the data


```{r}
balancedb1 = ovun.sample(Donation~.,data= b1, seed=123, method= "over")$data
```




```{r}
table(balancedb1$Donation)
```


### Create a training and testing split

```{r}
set.seed(1)
tr_ind = sample(nrow(balancedb1),.8*nrow(balancedb1), replace = F)
b1train = balancedb1[tr_ind,]
b1test = balancedb1[-tr_ind,]
```



\newpage

### Creating graphic models of important variables



```{r}
balancedb1 %>%
 filter(!(NeighborhoodCode %in% " ")) %>%
 ggplot() +
  aes(x = NeighborhoodCode, fill = Donation) +
  geom_bar() +
  scale_fill_hue(direction = 1) +
  theme_minimal()

```



```{r}
balancedb1 %>%
 filter(!(NeighborhoodCode %in% " ")) %>%
 ggplot() +
  aes(x = Donation, colour = Donation, weight = PercJapa) +
  geom_bar(fill = "#112446") +
  scale_color_hue(direction = 1) +
  labs(
    x = "Donation (yes/no)",
    y = "Count",
    title = "Japanese Donation"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```




```{r}
balancedb1 %>%
 filter(!(NeighborhoodCode %in% " ")) %>%
 ggplot() +
  aes(
    x = Donation,
    colour = Donation,
    group = Donation,
    weight = PercChin
  ) +
  geom_bar(fill = "#112446") +
  scale_color_hue(direction = 1) +
  labs(
    x = "Donation (yes/no)",
    y = "Count",
    title = "Chinese Donation"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```






\newpage

## Results


### Logistic Regression

|      Shows the accuracy that our logistic regression model has in predicting Yes and no Responses. The sensitivity of our model shows the accuracy we have for predicting "yes" responses and the specificity of our model shows the accuracy we have in predicting "no" responses. This model has an accuracy of 58.28%. We used .51 as our optimal cutoff point which gave us the best sensitivity and specificity which shows us that our model was accurate at predicting both yes and no responses.

```{r}
a1 = glm(formula = Donation ~ NeighborhoodCode + PercJapa + PercChin + AvgRent + IC3H + LFAF + MaxDol + LastDol, data = b1test , family = binomial)


car::vif(a1)
summary(a1)

predprob = predict.glm(a1, newdata = b1test, type = "response")
predclass_log = ifelse(predprob >= .51, "1", "0")
caret::confusionMatrix(as.factor(predclass_log), as.factor(b1test$Donation), positive = "1")
```




```{r}
optim_threshold(a1,b1test, b1test$Donation)
```


|       Running the optimal threshold function showed up that .51 was the best number to reach the highest sensitivity and specificity within our logistic regression model.




```{r}
m2.log = step(a1, direction = "backward")
summary(m2.log)

car::vif(m2.log)
```

|       This step function would take out any variables that could have a GVIF over 5 which would present multiculinarity in the dataset.We used this function to go backwards through our model and take out variables that were not significant. Our final Model consisted of the variables Percentage Japanese, percentage chinese, IC3H, LFAF, and LastDol.






### LDA Model

|       Shows the accuracy that our  LDA model has in predicting Yes and no Responses. This model is 58.38% accurate. The LDA model shows us a higher accuracy but due to the fact that we were not able to implent an optimal cutoff for the LDA model. This accuracy could be seen as false because the sensitivity and specificity are still lower that the logistic regression model. 


```{r}
m1.lda = lda(formula = as.factor(Donation) ~ NeighborhoodCode + PercJapa + PercChin + AvgRent + IC3H + LFAF + MaxDol + LastDol, data = b1test)
predclass_lda = predict(m1.lda, newdata = b1test)
caret::confusionMatrix(as.factor(predclass_lda$class),as.factor(b1test$Donation), positive = "1") 
```


### Random Forest 

```{r}

```





\newpage

### Decision Tree

|       Shows the accuracy that our  Decision Tree has in predicting Yes and no Responses. This model is 55.06% accurate. The Decision Tree model shows us a lower accuracy than the other two models but it is able to break down the variables  and show us the importance of each variable ranked.It also holds a sensitivity of 96.97% which shows that our model can predict "yes" responses but is relatively bad at predicting "no" responses.

```{r}
library(rpart.plot)
library(rpart)
library(caTools)
set.seed(123)
split = sample.split(b1$Donation, SplitRatio = 0.8)
b1train = subset(b1, split == TRUE)
b1test = subset(b1, split == FALSE)

tree.b1train <- rpart(formula = Donation ~ NeighborhoodCode + PercJapa + PercChin + AvgRent + IC3H + LFAF + MaxDol + LastDol,
                       data = balancedb1,control =rpart.control(minsplit =2500,minbucket=1,cp=0))

rpart.plot(tree.b1train) 

```




```{r}
pred_b1tree = predict(tree.b1train, newdata = b1test, type = "class")
confusionMatrix(b1test$Donation, pred_b1tree)
```

### Using VIP function to plot important variables
```{r}
vip(tree.b1train)
```



## Conclusions and Recommendations



\newpage

## References
