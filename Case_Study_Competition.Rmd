---
title: "Case Study competition"
author: "Thomas Farrell"
date: "3/23/2022"
output: pdf_document
toc : true
font size: 12pt

bibliography: citations.bib
---

\newpage

## Cover Page

Report by:Thomas Farrell, Leo Salazar, Mia Brito, Raneen Aljamal, Adrian Joshua


Primary Contact : Thomas Farrell (512) 400-9580

\newpage


## Executive Summary
|       This data set is collected directly from a fundraising agency and contains 95,412 records broken into 142 fields. The models shown related the efficiency of the different variables to predict if a consumer will donate to the campaign and if they donate it will predict how much they are likely to donate. We will use three different modeling techniques Linear Regression, Linear discriminant analysis(LDA) and Decision Tree modeling.

The main question to be answered is what type of individual will most likely donate? 
|       The donation can be monitored by factors such as neighborhood code, Their max donation, and the time of their last donation. In the data a donation is represented by the binary number 1. 
## Related Literature: 

|       When it comes to modeling approaches, it is crucial to pick the method that best satisfies your data’s needs. My team members and I have dove into the different types of modeling approaches to explore which method best fits our research. In our past case studies, we first explored logistic regression. As previously mentioned, Mitchell Dayton explains how logistic regression tackles the process of classifying problems. We also explored the use of a linear regression method. We learned that linear regression more commonly handles regression problems. To further enhance our knowledge of the different types of modeling approaches, we will be exploring the use of LDA and Decision Tree models in this case study.
|       Linear Discriminant Analysis, LDA, is another common modeling approach. LDA is commonly used to find a linear combination to separate more than two subjects or events. In Ricciardi, Valente, Edmund, Cantoni, Green, Fiorillo, Picone, Santini, and Cesarelli’s work, they used the LDA model to explore the use of a data mining technique to help clinicians in decision-making when it comes to Coronary Artery Disease. As previously mentioned, LDA models are used to separate two or more subjects. In this case study, they used the LDA model to classify the patients to help aid the decision making process. They concluded that this accelerated and enhanced the decision making process for doctors to choose the correct treatment for their patient. 
|       Similar to an LDA model, a Decision Tree model is used for classification purposes. A Decision Tree model builds regression by creating a model in a tree-like structure. It essentially makes the data set smaller by breaking it down into smaller subsets for an easier and quicker read. Stretch, Soares, and Moreira explored the use of a Decision Tree model when predicting the failure of students in University courses. They used the Decision Tree model to narrow down their data and make it smaller and were able to successfully explain the drop out phenomenon. 




## Data Anlaysis (Methodology)



|        Once we defined our variables in the data set, we changed all of our character variables to factor variables. We then created graphic models of significant variables in our data. This allowed us to visually see what people in our data could be beneficial to market to. When making our graphic representations we had to include Y as the fill for the bar chart or histogram. For the Categorical variables we decided to use a Bar chart to best represent our data. The fill of the "Donation" variable allowed us to show the relationship between the variable and if they would make a donation or not.















\newpage

## Appendix

### Loading in libraries
```{r setup, message=FALSE, echo=TRUE}
library(varImp)
library(caret)
library(lattice)
library(ggplot2)
library(gam)
library(caret)
library(ROCR)
library(ggmosaic)
library(prettydoc)
library(tinytex)
library(corrplot)
library(MASS)
library(tidyverse)
library(esquisse)
library(lubridate)
library(modeldata)
library(leaps)
library(randomForest)
source('/Users/thomasfarrell/Downloads/optim_threshold.R')

```


[@log1][@log2][@log3][@log4]



### Reading in data


```{r}
b1 = read.csv("/Users/thomasfarrell/Downloads/Customer_Analytics_TrainTest.csv", sep = ",")
```



### Changing character variables to factors

```{r}
b1$State = as.factor(b1$State)
b1$NeighborhoodCode = as.factor(b1$NeighborhoodCode)
b1$Gender = as.factor(b1$Gender)
b1$Donation = as.factor(b1$Donation)
b1 = subset(b1, !is.na(b1$Age))
b1 = subset(b1, !is.na(b1$NumberOfChild))
b1 = subset(b1, !is.na(b1$Income))
b1 = dplyr::select(b1, - PercMale)
b1 = dplyr::select(b1, - PercFemale)
b1 = dplyr::select(b1, - PercWhite)
b1 = dplyr::select(b1, - PercIsleAsian)
b1 = dplyr::select(b1, - AgeC1)
b1 = dplyr::select(b1, - AgeC2)
b1 = dplyr::select(b1, - AgeC3)
b1 = dplyr::select(b1, - AgeC4)
b1 = dplyr::select(b1, - AgeC5)
b1 = dplyr::select(b1, - AgeC6)
b1 = dplyr::select(b1, - AgeC7)
b1 = dplyr::select(b1, - HHN1)
b1 = dplyr::select(b1, - HHN2)
b1 = dplyr::select(b1, - HHN4)
b1 = dplyr::select(b1, - PercMarr)
b1 = dplyr::select(b1, - PercSingle)
b1 = dplyr::select(b1, - MedHomeVal)
b1 = dplyr::select(b1, - MedFamInc)
b1 = dplyr::select(b1, - PerCapInc)
b1 = dplyr::select(b1, - IC2H)
b1 = dplyr::select(b1, - IC1F)
b1 = dplyr::select(b1, - IC2F)
b1 = dplyr::select(b1, - IC3F)
b1 = dplyr::select(b1, - IC4F)
b1 = dplyr::select(b1, - IC5F)
b1 = dplyr::select(b1, - IC6F)
b1 = dplyr::select(b1, - IC9F)
b1 = dplyr::select(b1, - EMP3)
b1 = dplyr::select(b1, - EMP4)
b1 = dplyr::select(b1, - EMP8)
b1 = dplyr::select(b1, - EMP9)
b1 = dplyr::select(b1, - EMP13)
b1 = dplyr::select(b1, - EMP14)
b1 = dplyr::select(b1, - Edu1)
b1 = dplyr::select(b1, - Edu2)
b1 = dplyr::select(b1, - Edu3)
b1 = dplyr::select(b1, - Edu4)
b1 = dplyr::select(b1, - Edu5)
b1 = dplyr::select(b1, - Edu6)
b1 = dplyr::select(b1, - Edu7)
b1 = dplyr::select(b1, - PercForgnBr)
b1 = dplyr::select(b1, - LangEng)
b1 = dplyr::select(b1, - LangSpa)
b1 = dplyr::select(b1, - LangAs)
b1 = dplyr::select(b1, - LangOth)
b1 = dplyr::select(b1, - AmDonated)
b1 = dplyr::select(b1, - NumberOfChild)
b1 = dplyr::select(b1, - PercEmpState)


str(b1)
```


### Removing highly correlated variables

```{r}
b1_num = dplyr::select_if(b1, is.numeric)
M = cor(b1_num)
highcorr = findCorrelation(M, cutoff = .9, names = TRUE)
b1 = dplyr::select(b1, - highcorr)
```




### Checking summary of target variable

```{r}
summary(b1$Donation)
```


```{r}
set.seed(1)
tr_ind = sample(nrow(b1),.8*nrow(b1), replace = F)
b1train = b1[tr_ind,]
b1test = b1[-tr_ind,]
```


```{r}
a1 = glm(formula = Donation ~ . , data = b1 , family = binomial)


car::vif(a1)
summary(a1)

predprob = predict.glm(a1, newdata = b1test, type = "response")
predclass_log = ifelse(predprob >= .05, "1", "0")
caret::confusionMatrix(as.factor(predclass_log), as.factor(b1test$Donation), positive = "1")

```


```{r}
b1 %>%
 filter(!(NeighborhoodCode %in% " ")) %>%
 ggplot() +
  aes(x = NeighborhoodCode, fill = Donation) +
  geom_bar() +
  scale_fill_hue(direction = 1) +
  theme_minimal()

```



```{r}
b1 %>%
 filter(!(NeighborhoodCode %in% " ")) %>%
 ggplot() +
  aes(x = Donation, colour = Donation, weight = PercJapa) +
  geom_bar(fill = "#112446") +
  scale_color_hue(direction = 1) +
  labs(
    x = "Donation (yes/no)",
    y = "Count",
    title = "Japanese Donation"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```




```{r}
b1 %>%
 filter(!(NeighborhoodCode %in% " ")) %>%
 ggplot() +
  aes(
    x = Donation,
    colour = Donation,
    group = Donation,
    weight = PercChin
  ) +
  geom_bar(fill = "#112446") +
  scale_color_hue(direction = 1) +
  labs(
    x = "Donation (yes/no)",
    y = "Count",
    title = "Chinese Donation"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```


\newpage

## Results


### Logistic Regression

|      Shows the accuracy that our logistic regression model has in predicting Yes and no Responses. The sensitivity of our model shows the accuracy we have for predicting "yes" responses and the specificity of our model shows the accuracy we have in predicting "no" responses. This model has an accuracy of 62.1%. We used .05 as our optimal cutoff point which gave us the best sensitivity and specificity which shows us that our model was accurate at predicting both yes and no responses.

```{r}
a1 = glm(formula = Donation ~ NeighborhoodCode + PercJapa + PercChin + AvgRent + IC3H + LFAF + MaxDol + LastDol, data = b1 , family = binomial)


car::vif(a1)
summary(a1)

predprob = predict.glm(a1, newdata = b1test, type = "response")
predclass_log = ifelse(predprob >= .05, "1", "0")
caret::confusionMatrix(as.factor(predclass_log), as.factor(b1test$Donation), positive = "1")
```




```{r}
optim_threshold(a1,b1, b1$Donation)
```


|       Running the optimal threshold function showed up that .05 was the best number to reach the highest sensitivity and specificity within our logistic regression model.




```{r}
m2.log = step(a1, direction = "backward")
summary(m2.log)

car::vif(m2.log)
```

|       This step function would take out any variables that could have a GVIF over 5 which would present multiculinarity in the dataset.We used this function to go backwards through our model and take out variables that were not significant. Our final Model consisted of the variables Percentage Japanese, percentage chinese, IC3H, LFAF, and LastDol.


### LDA Model


```{r}
m1.lda = lda(formula = Donation ~ NeighborhoodCode + PercJapa + PercChin + AvgRent + IC3H + LFAF + MaxDol + LastDol, data = b1train)
predclass_lda = predict(m1.lda, newdata = b1test)
caret::confusionMatrix(as.factor(predclass_lda$class),as.factor(b1test$Donation), positive = "1") 
```




### Decision Tree


```{r}
library(rpart.plot)
library(rpart)
library(caTools)
set.seed(123)
split = sample.split(b1$Donation, SplitRatio = 0.8)
b1train = subset(b1, split == TRUE)
b1test = subset(b1, split == FALSE)

tree.b1train <- rpart(formula = Donation ~ NeighborhoodCode + PercJapa + PercChin + AvgRent + IC3H + LFAF + MaxDol + LastDol,
                       data = b1train,control =rpart.control(minsplit =180,minbucket=1,cp=0))

rpart.plot(tree.b1train)                     
```




```{r}
pred_b1tree = predict(tree.b1train, newdata = b1test)
summary(pred_b1tree)
```




## Conclusions and Recommendations



\newpage

## References
